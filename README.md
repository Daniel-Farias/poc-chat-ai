# POC Chat AI

Uma prova de conceito de chat com **streaming**, **rate limiting**, **tema claro/escuro**, e integraÃ§Ã£o com OpenAI.

---

## ğŸš€ InstalaÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone https://github.com/Daniel-Farias/poc-chat-ai.git
cd poc-chat-ai
````

2. Instale dependÃªncias no backend e frontend:

```bash
# Backend
cd backend
yarn install

# Frontend
cd ../frontend
yarn install
```

---

## ğŸ”‘ ConfiguraÃ§Ã£o de Ambiente

Para usar OpenAI:

1. Crie um arquivo `.env` no backend:

```env
OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX
MODEL="gpt-4o-mini"
```

---

## ğŸ’» Rodando localmente

### Backend (NestJS)

```bash
cd backend
yarn start:dev
```

* RodarÃ¡ em: `http://localhost:8080`

### Frontend (Next.js)

```bash
cd frontend
yarn dev
```

* RodarÃ¡ em: `http://localhost:3000`

---

## ğŸ§ª Testes

### Backend

```bash
cd backend
yarn test
```

* Inclui:

  * 3 testes unitÃ¡rios
  * 1 teste de integraÃ§Ã£o (endpoint SSE)
  * Teste de streaming (resposta em chunks)

---

## âš™ï¸ DecisÃµes TÃ©cnicas

### Por quÃª React + Vite no frontend?

* Setup extremamente rÃ¡pido para POC
* React Context + Hooks facilita gestÃ£o de estado global (`ChatContext`)
* Trade-off: Next.js poderia oferecer SSR, mas para streaming SSE local e prototipagem, React puro + Vite foi mais leve

### Por quÃª NestJS no backend?

* Estrutura modular e testÃ¡vel
* Suporte nativo a streaming SSE via EventSource
* Facilita injeÃ§Ã£o de dependÃªncias e organizaÃ§Ã£o de serviÃ§os (`ChatService`)

### Por quÃª OpenAI?

* Streaming real e confiÃ¡vel (`completion.stream = true`)
* DocumentaÃ§Ã£o completa e SDK oficial
* Claude poderia ser usado, mas OpenAI permitiu testes contÃ­nuos e fÃ¡cil integraÃ§Ã£o com SSE
* Trade-off: Claude teria limitaÃ§Ã£o de tokens por requisiÃ§Ã£o e menos exemplos de streaming

### ImplementaÃ§Ã£o do Streaming SSE

* Inicialmente, mensagens chegavam quebradas:

```
OlÃ¡
!
Como
posso
ajudar
vocÃª
```

* SoluÃ§Ã£o:

  * Yield **cada token individualmente** no backend
  * No frontend, concatenar no contexto (`ChatContext`) e renderizar como mensagem Ãºnica
  * Frontend mostra indicador `typing` enquanto SSE estÃ¡ ativo
  * Timeout de 15s para encerrar stream automaticamente

### Rate Limiting

* Limite: 10 mensagens por minuto
* Implementado com array de timestamps no backend
* Retorno SSE com evento `error` + tipo `RATE_LIMIT` para frontend mostrar mensagem de sistema
* Frontend exibe visualmente o aviso, sem tentar reconectar automaticamente

### Contexto e Estado Global

* `ChatContext` mantÃ©m:

  * Lista de mensagens (`user`, `assistant`, `system`)
  * Status de conexÃ£o (`idle`, `connecting`, `streaming`)
  * FunÃ§Ãµes: `addMessage`, `clearChat`, `sendMessage`
* `ChatClient` modulariza comunicaÃ§Ã£o SSE e lÃ³gica de timeout

### Error Handling

* Offline ou servidor down agora mostra mensagem de sistema no frontend:

  * `"ğŸ“¡ Sem conexÃ£o com a internet."`
  * `"âš  Servidor indisponÃ­vel."`
  * `"âŒ Erro ao conectar ao servidor."`

---

## ğŸ›  Desafios enfrentados

1. **Streaming SSE**

   * Problema: mensagens quebradas em vÃ¡rias linhas
   * SoluÃ§Ã£o: yield cada token individual no backend, concatenar no frontend

2. **Error Handling**

   * Problema: offline ou servidor indisponÃ­vel nÃ£o mostrava feedback
   * SoluÃ§Ã£o: SSE `onerror` + fetch health + mensagem de sistema

---

## ğŸ’¡ Melhorias Futuras

* [ ] Deploy completo em Vercel
* [ ] PersistÃªncia de histÃ³rico com Prisma
* [ ] Suporte a mÃºltiplos usuÃ¡rios simultÃ¢neos
* [ ] UI mais avanÃ§ada (avatars, rich text)
* [ ] Suporte a MÃºltiplos Chats

---

## ğŸ”— ReferÃªncias

* [OpenAI Node.js SDK](https://www.npmjs.com/package/openai)
* [NestJS SSE Docs](https://docs.nestjs.com/techniques/streaming)
